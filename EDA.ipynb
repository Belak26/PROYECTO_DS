{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# EDA\n",
    "This notebook conducts Exploratory Data Analysis (EDA) on the dataset intended for training a YOLO (You Only Look Once) object detection model. Through this analysis, we aim to gain valuable insights into the dataset's structure, quality, and patterns. EDA is a crucial step to inform preprocessing decisions and enhance the effectiveness of the subsequent model training process.\n",
    "\n",
    "Let's delve into the dataset, exploring its characteristics and uncovering essential patterns that will guide the development of an accurate and robust YOLO object detection model.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import  os\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "\n",
    "import re\n",
    "import keras_core as keras\n",
    "from keras_cv.bounding_box import convert_format\n",
    "#import tensorflow_io as tfio\n",
    "\n",
    "# Typing libraries\n",
    "from pathlib import Path\n",
    "from typing import List, Type"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Declare data path\n",
    "IMAGES_PATH = './construction-safety-2/train/images/'\n",
    "LABELS_PATH = './construction-safety-2/train/labels/'\n",
    "\n",
    "images_list = os.listdir(IMAGES_PATH)\n",
    "labels_list = os.listdir(LABELS_PATH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Function to open images\n",
    "def open_image(image_name: Path, size: tuple = (256,256)):\n",
    "    image_path = IMAGES_PATH+image_name\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize(size)\n",
    "    return img\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Images in a 256x256 format\n",
    "# Just for visualization for now\n",
    "\n",
    "for i in range(1):\n",
    "    image_file = images_list[i]\n",
    "    open_image(image_file).show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Image annotation system\n",
    "\n",
    "The yolo model has a particular way of organizing data annotations.\n",
    "\n",
    "First: We have a .txt file for each image in the dataset, each .txt file contains in each line an item. Each line is described as follows: the class (in numeric) and a bounding box associated with it.\n",
    "There can be multiple objects of the same class but in different lines of the .txt file.\n",
    "\n",
    "There is also a .yaml file that contains the names of the classes and the paths for the train, test, val directories."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.703 0.2866847826086957 0.118 0.09510869565217392\n",
      "3 0.477 0.5135869565217391 0.246 0.8804347826086957\n",
      "2 0.474 0.5203804347826086 0.224 0.48097826086956524\n",
      "3 0.28 0.5516304347826086 0.224 0.8804347826086957\n",
      "2 0.286 0.5706521739130435 0.236 0.42391304347826086\n",
      "4 0.703 0.5747282608695652 0.222 0.3125\n",
      "3 0.724 0.6209239130434783 0.236 0.7581521739130435\n",
      "0 0.46 0.14673913043478262 0.116 0.11956521739130435\n",
      "0 0.293 0.19972826086956522 0.122 0.14402173913043478\n"
     ]
    }
   ],
   "source": [
    "# A .txt file with the bounding boxes example\n",
    "\n",
    "with open(LABELS_PATH+'ppe_0001_jpg.rf.9508b03e3510a77a76fd74c9fe7441c9.txt') as txt_file:\n",
    "    text = txt_file.read()\n",
    "    print(text)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['helmet', 'no-helmet', 'no-vest', 'person', 'vest']\n"
     ]
    }
   ],
   "source": [
    "# Extract class_names using the yaml package\n",
    "with open('./construction-safety-2/data.yaml') as yaml_file:\n",
    "    yaml_file = yaml.load(yaml_file, SafeLoader)\n",
    "    classes = yaml_file['names']\n",
    "    print(classes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the images and labels directories have their files in the same order, we can simply call the first files to visualize the bounding boxes and classes on the images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def split_boxes(input_list: Type[List])-> List:\n",
    "    \"\"\"\n",
    "    Splits the input list into sublists, each containing 4 elements.\n",
    "\n",
    "    Args:\n",
    "    input_list (list): The input list to be split.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of lists, each containing 4 elements from the input list.\n",
    "    \"\"\"\n",
    "    return [input_list[i:i+4] for i in range(0, len(input_list), 4)]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "\n",
    "    # Read images\n",
    "    path_to_image = IMAGES_PATH+images_list[i]\n",
    "    img = tf.keras.utils.load_img(path_to_image)\n",
    "\n",
    "\n",
    "    # Read files and extract annotations\n",
    "    path_to_txt_file = LABELS_PATH+labels_list[i]\n",
    "    with open(path_to_txt_file) as txt_file:\n",
    "        labels = txt_file.readlines()\n",
    "\n",
    "    # Extract classes as names\n",
    "    image_classes = [classes[int(label[0])] for label in labels]\n",
    "\n",
    "    # Extract bboxes\n",
    "    bboxes = [label[2:-1].split(' ') for label in labels]\n",
    "    bboxes = [float(element) for box in bboxes for element in box]\n",
    "    bboxes = split_boxes(bboxes)\n",
    "\n",
    "    formated_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        bbox = tf.convert_to_tensor(bbox)\n",
    "        new_box = convert_format(bbox, 'xywh', 'xyxy')\n",
    "        formated_bboxes.append(new_box)\n",
    "    n_bboxes = len(formated_bboxes)\n",
    "    formated_bboxes = tf.reshape(tf.convert_to_tensor(formated_bboxes), [1, n_bboxes, 4])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "unable to open file: libtensorflow_io.so, from paths: ['C:\\\\Users\\\\omara\\\\Documents\\\\Omar\\\\Universidad\\\\ITESO\\\\Materias\\\\Proyecto de Ciencia de Datos\\\\Project\\\\PROYECTO_DS\\\\venv\\\\lib\\\\site-packages\\\\tensorflow_io\\\\python\\\\ops\\\\libtensorflow_io.so']\ncaused by: ['C:\\\\Users\\\\omara\\\\Documents\\\\Omar\\\\Universidad\\\\ITESO\\\\Materias\\\\Proyecto de Ciencia de Datos\\\\Project\\\\PROYECTO_DS\\\\venv\\\\lib\\\\site-packages\\\\tensorflow_io\\\\python\\\\ops\\\\libtensorflow_io.so not found']",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 32\u001B[0m\n\u001B[0;32m     28\u001B[0m n_bboxes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(formated_bboxes)\n\u001B[0;32m     29\u001B[0m formated_bboxes \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mreshape(tf\u001B[38;5;241m.\u001B[39mconvert_to_tensor(formated_bboxes), [\u001B[38;5;241m1\u001B[39m, n_bboxes, \u001B[38;5;241m4\u001B[39m])\n\u001B[1;32m---> 32\u001B[0m \u001B[43mtfio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexperimental\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdraw_bounding_boxes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformated_bboxes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mimage_classes\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Omar\\Universidad\\ITESO\\Materias\\Proyecto de Ciencia de Datos\\Project\\PROYECTO_DS\\venv\\lib\\site-packages\\tensorflow_io\\python\\experimental\\image_ops.py:40\u001B[0m, in \u001B[0;36mdraw_bounding_boxes\u001B[1;34m(images, boxes, texts, colors, name)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m colors \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     39\u001B[0m     colors \u001B[38;5;241m=\u001B[39m [[]]\n\u001B[1;32m---> 40\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcore_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mio_draw_bounding_boxes_v3\u001B[49m(images, boxes, colors, texts, name\u001B[38;5;241m=\u001B[39mname)\n",
      "File \u001B[1;32m~\\Documents\\Omar\\Universidad\\ITESO\\Materias\\Proyecto de Ciencia de Datos\\Project\\PROYECTO_DS\\venv\\lib\\site-packages\\tensorflow_io\\python\\ops\\__init__.py:88\u001B[0m, in \u001B[0;36mLazyLoader.__getattr__\u001B[1;34m(self, attrb)\u001B[0m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, attrb):\n\u001B[1;32m---> 88\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, attrb)\n",
      "File \u001B[1;32m~\\Documents\\Omar\\Universidad\\ITESO\\Materias\\Proyecto de Ciencia de Datos\\Project\\PROYECTO_DS\\venv\\lib\\site-packages\\tensorflow_io\\python\\ops\\__init__.py:84\u001B[0m, in \u001B[0;36mLazyLoader._load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_load\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m     83\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mod \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 84\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mod \u001B[38;5;241m=\u001B[39m \u001B[43m_load_library\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_library\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mod\n",
      "File \u001B[1;32m~\\Documents\\Omar\\Universidad\\ITESO\\Materias\\Proyecto de Ciencia de Datos\\Project\\PROYECTO_DS\\venv\\lib\\site-packages\\tensorflow_io\\python\\ops\\__init__.py:69\u001B[0m, in \u001B[0;36m_load_library\u001B[1;34m(filename, lib)\u001B[0m\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (tf\u001B[38;5;241m.\u001B[39merrors\u001B[38;5;241m.\u001B[39mNotFoundError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     68\u001B[0m         errs\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mstr\u001B[39m(e))\n\u001B[1;32m---> 69\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[0;32m     70\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munable to open file: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     71\u001B[0m     \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilename\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, from paths: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilenames\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mcaused by: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00merrs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     72\u001B[0m )\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: unable to open file: libtensorflow_io.so, from paths: ['C:\\\\Users\\\\omara\\\\Documents\\\\Omar\\\\Universidad\\\\ITESO\\\\Materias\\\\Proyecto de Ciencia de Datos\\\\Project\\\\PROYECTO_DS\\\\venv\\\\lib\\\\site-packages\\\\tensorflow_io\\\\python\\\\ops\\\\libtensorflow_io.so']\ncaused by: ['C:\\\\Users\\\\omara\\\\Documents\\\\Omar\\\\Universidad\\\\ITESO\\\\Materias\\\\Proyecto de Ciencia de Datos\\\\Project\\\\PROYECTO_DS\\\\venv\\\\lib\\\\site-packages\\\\tensorflow_io\\\\python\\\\ops\\\\libtensorflow_io.so not found']"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    # Read images\n",
    "    path_to_image = IMAGES_PATH+images_list[i]\n",
    "    img = keras.utils.load_img(path_to_image)\n",
    "    # Convert int8 to float32\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    # Append 1 extra dim as we are not using batches but the library expects them\n",
    "    img = tf.expand_dims(img, 0)\n",
    "\n",
    "    # Read files and extract annotations\n",
    "    path_to_txt_file = LABELS_PATH+labels_list[i]\n",
    "    with open(path_to_txt_file) as txt_file:\n",
    "        labels = txt_file.readlines()\n",
    "\n",
    "    # Extract classes as names\n",
    "    image_classes = [classes[int(label[0])] for label in labels]\n",
    "     # Extract bboxes\n",
    "    bboxes = [label[2:-1].split(' ') for label in labels]\n",
    "    bboxes = [float(element) for box in bboxes for element in box]\n",
    "    bboxes = split_boxes(bboxes)\n",
    "\n",
    "    formated_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        bbox = tf.convert_to_tensor(bbox)\n",
    "        new_box = convert_format(bbox, 'xywh', 'xyxy')\n",
    "        formated_bboxes.append(new_box)\n",
    "    n_bboxes = len(formated_bboxes)\n",
    "    formated_bboxes = tf.reshape(tf.convert_to_tensor(formated_bboxes), [1, n_bboxes, 4])\n",
    "\n",
    "\n",
    "    tfio.experimental.image.draw_bounding_boxes(img, formated_bboxes, [image_classes])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
